
# ğŸ“Š Complete Project Overview

## ğŸ¯ What is This Project?

**AI-Based Document Understanding & Content Generation System** - A complete RAG (Retrieval-Augmented Generation) system that:

- **Understands your documents** by processing PDF, TXT, and MD files
- **Generates accurate content** grounded in your documents (summaries, blog posts, LinkedIn posts)
- **Works 100% locally** - No API keys, no internet needed after setup
- **Protects your privacy** - All data stays on your machine

---

## ğŸ—ï¸ System Architecture

### High-Level Flow

```
ğŸ“„ User Documents (PDF/TXT/MD)
    â†“
âœ‚ï¸ Document Chunking (512 words per chunk, 64 overlap)
    â†“
ğŸ”¢ Embedding Generation (Sentence Transformers)
    â†“
ğŸ’¾ Vector Storage (FAISS Index)
    â†“
ğŸ” Query Processing
    â†“
ğŸ“Š Semantic Search (Find Top-K Relevant Chunks)
    â†“
ğŸ¤– LLM Generation (Qwen2.5 with Retrieved Context)
    â†“
âœ¨ Generated Content (Summary/Blog/LinkedIn/Image)
```

### Detailed Components

1. **Document Ingestion** (`rag_local.py` â†’ `build_store()`)
   - Reads PDF/TXT/MD files
   - Splits into chunks (512 words, 64 overlap)
   - Creates embeddings using Sentence Transformers
   - Stores in FAISS vector database

2. **Query Processing** (`rag_local.py` â†’ `run_query()`)
   - User asks a question
   - System retrieves top-K relevant chunks
   - Passes chunks as context to LLM
   - Generates grounded response

3. **Content Generation** (`rag_local.py` â†’ Content Templates)
   - **Summary**: 2-4 sentence concise overview
   - **Blog Intro**: Engaging opening paragraphs
   - **LinkedIn Post**: Professional social media content
   - **Image**: Context-aware image generation

---

## ğŸ“ Project Structure

```
sih/
â”œâ”€â”€ app.py                    # ğŸŒ Streamlit Web Interface (600 lines)
â”‚   â”œâ”€â”€ Document upload UI
â”‚   â”œâ”€â”€ 5 Content Generation Tabs:
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ Summary
â”‚   â”‚   â”œâ”€â”€ ğŸ“ Blog Post
â”‚   â”‚   â”œâ”€â”€ ğŸ’¼ LinkedIn Post
â”‚   â”‚   â”œâ”€â”€ ğŸ–¼ï¸ Image Generation
â”‚   â”‚   â””â”€â”€ ğŸ” Custom Query
â”‚   â””â”€â”€ Session state management
â”‚
â”œâ”€â”€ rag_local.py              # âš™ï¸ Core RAG Pipeline (428 lines)
â”‚   â”œâ”€â”€ build_store()         # Document ingestion
â”‚   â”œâ”€â”€ load_store()          # Load vector database
â”‚   â”œâ”€â”€ retrieve()           # Semantic search
â”‚   â”œâ”€â”€ run_query()           # RAG query execution
â”‚   â”œâ”€â”€ generate_image_from_context()  # Image generation
â”‚   â””â”€â”€ Content templates (summary, blog, linkedin, general)
â”‚
â”œâ”€â”€ requirements_rag.txt      # ğŸ“¦ Dependencies (11 packages)
â”‚   â”œâ”€â”€ transformers
â”‚   â”œâ”€â”€ sentence-transformers
â”‚   â”œâ”€â”€ faiss-cpu
â”‚   â”œâ”€â”€ streamlit
â”‚   â”œâ”€â”€ torch
â”‚   â”œâ”€â”€ diffusers
â”‚   â””â”€â”€ pypdf
â”‚
â”œâ”€â”€ setup.bat                 # ğŸš€ Windows setup script
â”‚
â”œâ”€â”€ README.md                 # ğŸ“– Complete documentation
â”‚
â”œâ”€â”€ docs/                     # ğŸ“‚ User documents folder
â”‚   â””â”€â”€ (Upload PDF/TXT/MD here)
â”‚
â””â”€â”€ rag_store/                # ğŸ’¾ Generated vector store
    â”œâ”€â”€ index.faiss           # FAISS vector index
    â”œâ”€â”€ chunks.json           # Document chunks
    â”œâ”€â”€ meta.json             # Chunk metadata
    â””â”€â”€ config.json           # Configuration
```

---

## ğŸ› ï¸ Technology Stack

| Layer | Technology | Purpose |
|-------|-----------|---------|
| **Frontend** | Streamlit | Web UI framework |
| **Embeddings** | Sentence Transformers (`all-MiniLM-L6-v2`) | Convert text to vectors |
| **Vector DB** | FAISS (Facebook AI Similarity Search) | Fast similarity search |
| **Language Model** | Qwen2.5-0.5B-Instruct | Text generation |
| **Image Model** | Stable Diffusion v1.5 | Image generation |
| **PDF Reader** | pypdf | Extract text from PDFs |
| **ML Framework** | PyTorch | Model execution |

### Model Details

- **Embedding Model**: `sentence-transformers/all-MiniLM-L6-v2`
  - Size: ~80MB
  - Dimensions: 384
  - Speed: Fast, optimized for CPU

- **LLM**: `Qwen/Qwen2.5-0.5B-Instruct`
  - Size: ~1GB
  - Parameters: 500M
  - Speed: Fast inference, good quality

- **Image Model**: `runwayml/stable-diffusion-v1-5`
  - Size: ~4GB (first download)
  - Quality: High-quality image generation

---

## ğŸ”‘ Key Features

### 1. **Document Processing**
- âœ… Supports PDF, TXT, MD files
- âœ… Automatic chunking (512 words, 64 overlap)
- âœ… Memory-optimized (handles large documents)
- âœ… Batch processing for multiple files

### 2. **Content Generation Types**

#### ğŸ“„ Summary
- **Output**: 2-4 sentence concise summary
- **Features**: 
  - Describes document purpose
  - Lists key sections
  - No source citations in output
  - Professional abstract style

#### ğŸ“ Blog Post Introduction
- **Output**: Engaging blog opening
- **Features**:
  - Hooks the reader
  - Sets up topic
  - Professional yet accessible

#### ğŸ’¼ LinkedIn Post
- **Output**: Professional social media content
- **Features**:
  - 2-3 paragraphs
  - Actionable takeaways
  - Professional hashtags

#### ğŸ–¼ï¸ Image Generation
- **Output**: Context-aware images
- **Features**:
  - Based on document content
  - Uses Stable Diffusion
  - Customizable inference steps

#### ğŸ” Custom Query
- **Output**: Answer to any question
- **Features**:
  - Flexible content types
  - Adjustable parameters
  - Source attribution

### 3. **User Interface**

#### Web Interface (Streamlit)
- **Sidebar**: Document upload and management
- **Main Area**: 5 tabs for different content types
- **Features**:
  - Drag-and-drop file upload
  - Real-time processing progress
  - Source attribution display
  - Retrieved chunks viewer

#### Command Line Interface
- **Ingest**: `python rag_local.py ingest --docs ./docs --store ./rag_store`
- **Query**: `python rag_local.py query --question "..." --store ./rag_store`
- **Options**: Content type, max tokens, top-K chunks

### 4. **Privacy & Performance**
- âœ… 100% local processing
- âœ… No API calls
- âœ… No data sent to external servers
- âœ… Offline capable after setup
- âœ… Memory-optimized for large documents

---

## ğŸ’» Code Structure

### `app.py` (600 lines)

**Main Components:**
1. **Page Configuration** (lines 19-24)
   - Streamlit page setup
   - Theme configuration

2. **Custom CSS** (lines 27-139)
   - Light theme styling
   - Sidebar visibility fixes
   - Responsive design

3. **Session State** (lines 141-153)
   - Store vector database in memory
   - Track loaded state
   - Cache encoder and index

4. **Document Management** (lines 160-288)
   - File uploader
   - Document processing
   - Store loading

5. **Content Generation Tabs** (lines 300-547)
   - Summary tab (lines 300-346)
   - Blog Post tab (lines 348-394)
   - LinkedIn Post tab (lines 396-445)
   - Image Generation tab (lines 447-491)
   - Custom Query tab (lines 494-547)

### `rag_local.py` (428 lines)

**Main Functions:**

1. **`build_store(docs_dir, store_dir, embed_model)`** (lines 118-207)
   - Loads documents
   - Chunks text
   - Generates embeddings
   - Creates FAISS index
   - Saves to disk

2. **`load_store(store_dir)`** (lines 210-217)
   - Loads FAISS index
   - Loads chunks and metadata
   - Initializes encoder

3. **`retrieve(query, index, encoder, chunks, meta, top_k)`** (lines 220-226)
   - Encodes query
   - Searches FAISS index
   - Returns top-K chunks with scores

4. **`run_query(question, store_dir, ...)`** (lines 301-343)
   - Retrieves relevant chunks
   - Formats prompt
   - Generates response
   - Cleans output

5. **`generate_image_from_context(retrieved, ...)`** (lines 346-380)
   - Extracts key phrases
   - Creates image prompt
   - Generates image with Stable Diffusion

**Content Templates** (lines 38-66):
- `summary`: Concise 2-4 sentence summary
- `blog_intro`: Engaging blog opening
- `linkedin_post`: Professional social media content
- `general`: Q&A format

---

## ğŸš€ Usage Examples

### Example 1: Generate Summary

**Input Document**: Research paper about AI

**Process**:
1. Upload PDF via web interface
2. Click "Process Documents"
3. Go to Summary tab
4. Click "Generate Summary"

**Output**:
```
This research paper presents a comprehensive analysis of artificial 
intelligence applications in healthcare. The document covers key 
sections including introduction to AI in medicine, machine learning 
algorithms for diagnosis, ethical considerations, and future 
implications. It demonstrates how AI technologies can enhance 
healthcare delivery while addressing privacy and accuracy concerns.
```

**Sources**: ğŸ“„ research_paper.pdf (Chunks 5, 12, 18)

---

### Example 2: Generate LinkedIn Post

**Input**: Company annual report

**Command**:
```bash
python rag_local.py query \
  --question "Create a LinkedIn post" \
  --store ./rag_store \
  --content-type linkedin_post \
  --max-tokens 256
```

**Output**:
```
ğŸ‰ Excited to share our company's annual achievements! 

This year we've seen remarkable growth in innovation and customer 
satisfaction. Our team's dedication to excellence has driven 
significant improvements across all departments.

Key highlights include 30% revenue growth, expansion into 5 new 
markets, and recognition as a top workplace. Looking forward to 
building on this momentum in the coming year!

#BusinessGrowth #Innovation #TeamSuccess #AnnualReport
```

---

### Example 3: Custom Query

**Question**: "What are the main findings?"

**Output**: Detailed answer based on retrieved document chunks with source attribution.

---

## ğŸ“Š Data Flow Example

### Step-by-Step: Generating a Summary

1. **User Action**: Uploads `document.pdf` and clicks "Generate Summary"

2. **Document Processing**:
   ```
   document.pdf (10 pages)
   â†’ Extract text (pypdf)
   â†’ Clean text (remove extra whitespace)
   â†’ Chunk into 25 chunks (512 words each)
   â†’ Generate 25 embeddings (384 dimensions each)
   â†’ Store in FAISS index
   ```

3. **Query Processing**:
   ```
   Question: "Summarize the main points"
   â†’ Encode question to vector
   â†’ Search FAISS (cosine similarity)
   â†’ Retrieve top 5 chunks (scores: 0.89, 0.85, 0.82, 0.78, 0.75)
   ```

4. **Generation**:
   ```
   Retrieved chunks â†’ Format prompt
   â†’ Pass to Qwen2.5-0.5B-Instruct
   â†’ Generate summary (256 tokens)
   â†’ Clean output (remove source refs)
   â†’ Display to user
   ```

5. **Output Display**:
   - Summary text
   - Sources: ğŸ“„ document.pdf (Chunks 3, 7, 12, 15, 22)
   - Expandable chunk viewer

---

## ğŸ¨ UI Features

### Web Interface Highlights

1. **Sidebar**:
   - File uploader (PDF, TXT, MD)
   - Process Documents button
   - Load existing store option
   - Status indicator

2. **Main Tabs**:
   - **Summary**: Slider for max tokens (128-1024), Top-K selector
   - **Blog Post**: Similar controls, blog-specific generation
   - **LinkedIn Post**: Shorter max tokens (128-512)
   - **Image Generation**: Top-K chunks, inference steps slider
   - **Custom Query**: Full flexibility

3. **Output Display**:
   - Generated content in styled box
   - Sources section with emojis
   - Expandable chunk viewer
   - Relevance scores

---

## ğŸ”§ Configuration Options

### Adjustable Parameters

- **Max Tokens**: 128-1024 (controls output length)
- **Top-K Chunks**: 1-20 (number of retrieved chunks)
- **Content Type**: summary, blog_intro, linkedin_post, general
- **Inference Steps**: 10-50 (for image generation)

### Model Swapping

You can change models in `rag_local.py`:

```python
# For better quality (requires more resources)
DEFAULT_LLM = "mistralai/Mistral-7B-Instruct-v0.2"
DEFAULT_EMBED_MODEL = "sentence-transformers/all-mpnet-base-v2"

# For faster processing (current defaults)
DEFAULT_LLM = "Qwen/Qwen2.5-0.5B-Instruct"
DEFAULT_EMBED_MODEL = "sentence-transformers/all-MiniLM-L6-v2"
```

---

## ğŸ“ˆ Performance Characteristics

### Memory Usage
- **Embedding Model**: ~200MB RAM
- **LLM (Qwen2.5-0.5B)**: ~2GB RAM
- **FAISS Index**: ~50MB per 1000 chunks
- **Stable Diffusion**: ~4GB RAM (or VRAM if GPU)

### Processing Speed
- **Document Ingestion**: ~1-2 seconds per page
- **Query Processing**: ~2-5 seconds (CPU)
- **Image Generation**: ~30-60 seconds (CPU), ~5-10 seconds (GPU)

### Scalability
- **Max Document Size**: 500K characters (auto-truncated)
- **Max Chunks**: 1000 chunks per document
- **Batch Processing**: Supports multiple documents

---

## ğŸ¯ Key Advantages

1. **No API Costs**: All processing is local
2. **Privacy**: Data never leaves your machine
3. **Offline**: Works without internet after setup
4. **Accurate**: Grounded in actual documents (reduces hallucinations)
5. **Flexible**: Multiple content types from same pipeline
6. **Open Source**: Uses open-source models and libraries
7. **Easy to Use**: Both web UI and CLI interfaces

---

## ğŸ”„ Workflow Summary

### Typical User Journey

1. **Setup** (one-time):
   ```bash
   pip install -r requirements_rag.txt
   ```

2. **Add Documents**:
   - Place PDF/TXT/MD files in `docs/` folder
   - OR upload via web interface

3. **Process Documents**:
   - Web: Click "Process Documents"
   - CLI: `python rag_local.py ingest --docs ./docs --store ./rag_store`

4. **Generate Content**:
   - Web: Select tab â†’ Adjust parameters â†’ Generate
   - CLI: `python rag_local.py query --question "..." --store ./rag_store`

5. **View Results**:
   - Generated content
   - Source attribution
   - Retrieved chunks (optional)

---

## ğŸ“ Learning Resources

### Understanding RAG
- **Retrieval**: Finding relevant information from documents
- **Augmentation**: Adding context to the prompt
- **Generation**: Creating response based on context

### Key Concepts
- **Embeddings**: Vector representations of text
- **Semantic Search**: Finding similar meaning, not just keywords
- **Chunking**: Breaking documents into manageable pieces
- **Vector Database**: Fast similarity search (FAISS)

---

## ğŸš¦ Project Status

âœ… **Complete Features**:
- Document ingestion (PDF, TXT, MD)
- Vector storage (FAISS)
- Semantic search
- Content generation (Summary, Blog, LinkedIn)
- Image generation
- Web interface
- CLI interface
- Source attribution
- Memory optimization

ğŸ”„ **Future Enhancements** (Potential):
- Support for more file formats (DOCX, HTML)
- Multi-language support
- Advanced chunking strategies
- Model fine-tuning capabilities
- Export generated content
- Batch processing UI

---

## ğŸ“ Quick Reference

### Essential Commands

```bash
# Install
pip install -r requirements_rag.txt

# Run Web App
streamlit run app.py

# Process Documents (CLI)
python rag_local.py ingest --docs ./docs --store ./rag_store

# Generate Summary (CLI)
python rag_local.py query --question "Summarize" --store ./rag_store --content-type summary

# Generate LinkedIn Post (CLI)
python rag_local.py query --question "LinkedIn post" --store ./rag_store --content-type linkedin_post --max-tokens 256
```

### File Locations

- **Documents**: `docs/` folder
- **Vector Store**: `rag_store/` folder (auto-created)
- **Web App**: `app.py`
- **RAG Pipeline**: `rag_local.py`
- **Dependencies**: `requirements_rag.txt`

---

**This project demonstrates a complete, production-ready RAG system that works entirely offline with no API dependencies!** ğŸš€

